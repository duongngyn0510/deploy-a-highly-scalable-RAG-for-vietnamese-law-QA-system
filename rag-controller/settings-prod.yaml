server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8080}

llm:
  mode: ${LLM_MODE:nvidia_nim}

embedding:
  mode: ${EMBEDDING_MODE:text_embeddings_inference}

vectorstore:
  database: ${VECTOR_STORE:weaviate}

vllm:
  vllm_endpoint: ${VLLM_ENDPOINT:http://localhost:6017/v1/completions}
  llm_model: ${LLM_MODEL:Viet-Mistral/Vistral-7B-Chat}
  prompt_style: default
  temperature: 1
  max_tokens: ${VLLM_MAX_TOKENS:256}

text_embeddings_inference:
  text_embeddings_inference_endpoint: ${TEXT_EMBEDDINGS_INFERENCE_ENDPOINT:http://localhost:8081}
  timeout: 60

weaviate:
  weaviate_endpoint: ${WEAVIATE_ENDPOINT:http://localhost:9091}
  index_name: Law07032024

rag:
  # This value controls how many "top" documents the RAG returns to use in the context.
  similarity_top_k: ${SIMILARITY_TOP_K:20}
  
  hybrid_retriever:
    enabled: true

  rerank:
    enabled: ${RERANK_ENABLED:false}
    top_n: ${RERANK_TOP_N:5}
    mode: model_api

translation:
  enabled: ${TRANSLATION_ENABLED:false}
  mode: triton_envit5
    
translation_tokenizer:
  pretrained_model_name_or_path: VietAI/envit5-translation
  padding: true

envit5_translation_triton_server:
  triton_server_endpoint: ${TRITON_SERVER_ENDPOINT:-}
  triton_model_name: ${TRITON_MODEL_NAME:-}
  src_lang: vi
  tag_lang: en

model_api_rerank:
  model_api_endpoint: http://localhost:8084/rerank

auto_redirect:
  enabled: ${AUTO_REDIRECT_ENABLED:false}
  llm_api_provider: ${AUTO_REDIRECT_LLM_API_PROVIDER:nvidia_nim}
  threshold: ${AUTO_REDIRECT_THRESHOLD:150}
  prometheus_url: ${PROMETHEUS_URL:http://prometheus-server.monitoring.svc.cluster.local:9090}
  prometheus_query: sum(increase(nginx_ingress_controller_requests{}[2m]))

nvidia_nim:
  model: meta/llama3-70b-instruct
  api_base: https://integrate.api.nvidia.com/v1
  api_key: ${API_KEY:}
  temperature: 0.5
  top_p: 1
  max_tokens: 1024