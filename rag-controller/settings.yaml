# The default configuration file.

server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: false
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="
  
data:
  local_data_folder: local_data/src

ui:
  enabled: true
  path: /
  default_chat_system_prompt: >
    Bạn là một trợ lí Tiếng Việt nhiệt tình và trung thực. 
    Hãy luôn trả lời một cách hữu ích nhất có thể, đồng thời giữ an toàn.
    Câu trả lời của bạn không nên chứa bất kỳ nội dung gây hại, phân biệt chủng tộc, phân biệt giới tính, độc hại, nguy hiểm hoặc bất hợp pháp nào. 
    Hãy đảm bảo rằng các câu trả lời của bạn không có thiên kiến xã hội và mang tính tích cực.Nếu một câu hỏi không có ý nghĩa hoặc không hợp lý về mặt thông tin, hãy giải thích tại sao thay vì trả lời một điều gì đó không chính xác. 
    Nếu bạn không biết câu trả lời cho một câu hỏi, hãy trẳ lời là bạn không biết và vui lòng không chia sẻ thông tin sai lệch
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: llamacpp
  # Should be matching the selected model
  max_new_tokens: 512
  context_window: 3900
  tokenizer: mistralai/Mistral-7B-Instruct-v0.2
  temperature: 0.1      # The temperature of the model. Increasing the temperature will make the model answer more creatively. A value of 0.1 would be more factual. (Default: 0.1)

rag:
  # This value controls how many "top" documents the RAG returns to use in the context.
  similarity_top_k: 10
  
  hybrid_retriever:
    enabled: true

  rerank:
    enabled: false
    top_n: 3
    mode: model_api

translation:
  enabled: false
  mode: triton_envit5

translation_tokenizer:
  pretrained_model_name_or_path: VietAI/envit5-translation
  padding: true

nllb_translation_triton_server:
  triton_server_endpoint: localhost:8001
  triton_model_name: nllb-200-distilled-600M-fp16
  src_lang: vie_Latn
  tag_lang: eng_Latn

envit5_translation_triton_server:
  triton_server_endpoint: localhost:8001
  triton_model_name: envit5
  src_lang: vi
  tag_lang: en

model_api_rerank:
  model_api_endpoint: ${MODEL_API_ENDPOINT:}

local_rerank:
  model: cross-encoder/ms-marco-MiniLM-L-2-v2

text_embeddings_inference_rerank:
  text_embeddings_inference_endpoint: ${TEXT_EMBEDDINGS_INFERENCE_ENDPOINT:}

llamacpp:
  prompt_style: "mistral"
  llm_hf_repo_id: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
  llm_hf_model_file: mistral-7b-instruct-v0.2.Q4_K_M.gguf
  tfs_z: 1.0            # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting
  top_k: 40             # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
  top_p: 1.0            # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
  repeat_penalty: 1.1   # Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)

embedding:
  # Should be matching the value above in most cases
  mode: huggingface
  ingest_mode: simple
  embed_dim: 384 # 384 is for BAAI/bge-small-en-v1.5

huggingface:
  embedding_hf_model_name: BAAI/bge-small-en-v1.5

vectorstore:
  database: qdrant

nodestore:
  database: simple

qdrant:
  path: local_data/src/qdrant

postgres:
  host: localhost
  port: 5432
  database: postgres
  user: postgres
  password: postgres
  schema_name: src

weaviate:
  weaviate_endpoint: localhost:9090
  api_key: ${WEAVIATE_API_KEY:}
  index_name: Law07032024

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo

ollama:
  llm_model: llama2
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  # change if your embedding model runs on another ollama
  keep_alive: 5m
  request_timeout: 120.0

azopenai:
  api_key: ${AZ_OPENAI_API_KEY:}
  azure_endpoint: ${AZ_OPENAI_ENDPOINT:}
  embedding_deployment_name: ${AZ_OPENAI_EMBEDDING_DEPLOYMENT_NAME:}
  llm_deployment_name: ${AZ_OPENAI_LLM_DEPLOYMENT_NAME:}
  api_version: "2023-05-15"
  embedding_model: text-embedding-ada-002
  llm_model: gpt-35-turbo

vllm:
  vllm_endpoint: ${VLLM_ENDPOINT:}
  llm_model: ${LLM_MODEL:Viet-Mistral/Vistral-7B-Chat}
  prompt_style: default
  temperature: 1
  max_tokens: 256

text_embeddings_inference:
  text_embeddings_inference_endpoint: http://127.0.0.1:8080
  timeout: 60

auto_redirect:
  enabled: false
  llm_api_provider: nvidia_nim
  threshold: 150
  prometheus_url: http://prometheus-server.monitoring.svc.cluster.local:9090
  prometheus_query: sum(increase(nginx_ingress_controller_requests{}[2m]))

nvidia_nim:
  model: meta/llama3-70b-instruct
  api_base: https://integrate.api.nvidia.com/v1
  api_key: ${API_KEY:}
  temperature: 0.5
  top_p: 1
  max_tokens: 1024