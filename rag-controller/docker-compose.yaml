# docker compose for local deployment

version: '3.4'
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: semitechnologies/weaviate:1.23.9
    ports:
    - 9091:8080
    - 50051:50051
    volumes:
    - ./weaviate_data:/var/lib/weaviate
    # restart: on-failure:0
    # restart: always
    environment:
      QUERY_DEFAULTS_LIMIT: 100
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DISK_USE_READONLY_PERCENTAGE: 100
      DISK_USE_WARNING_PERCENTAGE: 100
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
      
  embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
    command: ["--model-id", "nhatminh/vietnamese_bi_encoder"]
    # runtime: nvidia
    volumes:
      - /home/duongntd/data:/data
    ports:
      - 8082:80

  llm:
    image: vllm/vllm-openai:v0.2.2
    entrypoint: ["python3", "vllm.entrypoints.api_server"]
    runtime: nvidia
    environment:
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "6017:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: ["--model", "Viet-Mistral/Vistral-7B-Chat", "--gpu-memory-utilization=0.8"]
    ipc: "host"

  reranker:
    image: ghcr.io/huggingface/text-embeddings-inference:1.2
    runtime: nvidia
    ports:
      - "6018:80"
    volumes:
      - /home/duongntd2/:/data
    command: ["--model-id", "BAAI/bge-reranker-v2-m3"]
