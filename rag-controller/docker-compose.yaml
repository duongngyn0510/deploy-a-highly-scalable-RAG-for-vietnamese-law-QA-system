# docker compose for local test

version: '3.4'
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: semitechnologies/weaviate:1.23.9
    ports:
    - 9091:8080
    - 50051:50051
    volumes:
    - ./weaviate_data:/var/lib/weaviate
    # restart: on-failure:0
    # restart: always
    environment:
      QUERY_DEFAULTS_LIMIT: 100
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DISK_USE_READONLY_PERCENTAGE: 100
      DISK_USE_WARNING_PERCENTAGE: 100
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
      
  embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
    command: ["--model-id", "nhatminh/vietnamese_bi_encoder"]
    # runtime: nvidia
    volumes:
      - /home/duongntd/data:/data
    ports:
      - 8082:80
    environment:
      HUGGING_FACE_HUB_TOKEN: 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
  
  # legal-rag:
  #   # build:
  #   #   dockerfile: Dockerfile
  #   image: legal-rag:v1.0
  #   ports:
  #     - 8099:8080
  #   depends_on:
  #     - embeddings
  #     - weaviate
  #   environment:
  #     PROFILES: docker
  #     RAG_WORKERS: 2
  #     # PORT: 8089
  #     LLM_MODE: vllm
  #     TRANSLATION_ENABLED: false
  #     EMBEDDING_MODE: text_embeddings_inference
  #     VECTOR_STORE: weaviate
  #     VLLM_ENDPOINT: http://103.145.79.20:6017/generate
  #     LLM_MODEL: Viet-Mistral/Vistral-7B-Chat
  #     TEXT_EMBEDDINGS_INFERENCE_ENDPOINT: http://embeddings:80
  #     WEAVIATE_ENDPOINT: http://weaviate:8080
  #     TRITON_SERVER_ENDPOINT: localhost:8001
  #     TRITON_MODEL_NAME: envit5

